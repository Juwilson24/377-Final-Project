{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9732512-2818-4ed0-b8c6-2014d80688ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'input_data1/hmda_2007_nationwide_first-lien-owner-occupied-1-4-family-records_labels.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m zipfile\u001b[38;5;241m.\u001b[39mZipFile(zip_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n\u001b[1;32m     13\u001b[0m         zip_ref\u001b[38;5;241m.\u001b[39mextractall(extract_to)\n\u001b[0;32m---> 15\u001b[0m unzip_file(base_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhmda_2007_nationwide_first-lien-owner-occupied-1-4-family-records_labels.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m, base_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2007_data/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m unzip_file(base_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhmda_2017_nationwide_first-lien-owner-occupied-1-4-family-records_labels (1).zip\u001b[39m\u001b[38;5;124m\"\u001b[39m, base_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2017_data/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m unzip_file(base_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2023_combined_mlar.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m, base_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2023_data/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m, in \u001b[0;36munzip_file\u001b[0;34m(zip_path, extract_to)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munzip_file\u001b[39m(zip_path, extract_to):\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m zipfile\u001b[38;5;241m.\u001b[39mZipFile(zip_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n\u001b[1;32m     13\u001b[0m         zip_ref\u001b[38;5;241m.\u001b[39mextractall(extract_to)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/zipfile.py:1284\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1284\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mopen(file, filemode)\n\u001b[1;32m   1285\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1286\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'input_data1/hmda_2007_nationwide_first-lien-owner-occupied-1-4-family-records_labels.zip'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# === Set up paths ===\n",
    "base_path = \"input_data1/\"\n",
    "output_path = \"cleaned_data/\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# === Step 1: Unzip files ===\n",
    "def unzip_file(zip_path, extract_to):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "unzip_file(base_path + \"hmda_2007_nationwide_first-lien-owner-occupied-1-4-family-records_labels.zip\", base_path + \"2007_data/\")\n",
    "unzip_file(base_path + \"hmda_2017_nationwide_first-lien-owner-occupied-1-4-family-records_labels (1).zip\", base_path + \"2017_data/\")\n",
    "unzip_file(base_path + \"2023_combined_mlar.zip\", base_path + \"2023_data/\")\n",
    "\n",
    "# === Step 2: Chunked processing for 2007/2017 ===\n",
    "def process_hmda_file(folder, output_filename, year, delimiter, state_codes):\n",
    "    chunk_size = 100000\n",
    "    first_chunk = True\n",
    "    columns_to_keep = [\n",
    "        'year', 'state_code', 'county_code', 'census_tract_number',\n",
    "        'loan_amount_000s', 'applicant_income_000s',\n",
    "        'property_value', 'loan_purpose', 'lien_status'\n",
    "    ]\n",
    "\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith(\".csv\") or file.endswith(\".txt\"):\n",
    "            path = os.path.join(folder, file)\n",
    "            break\n",
    "\n",
    "    for chunk in pd.read_csv(path, sep=delimiter, chunksize=chunk_size, low_memory=False):\n",
    "        chunk.columns = [col.strip().lower() for col in chunk.columns]\n",
    "        if 'state_code' not in chunk.columns:\n",
    "            continue\n",
    "        chunk = chunk[chunk['state_code'].astype(str).isin(state_codes)]\n",
    "        filters = (\n",
    "            (chunk['loan_type'] == 1) if 'loan_type' in chunk.columns else True\n",
    "        ) & (\n",
    "            (chunk['property_type'] == 1) if 'property_type' in chunk.columns else True\n",
    "        ) & (\n",
    "            (chunk.get('owner_occupancy', chunk.get('occupancy')) == 1)\n",
    "        ) & (\n",
    "            (chunk['action_taken'] == 1) if 'action_taken' in chunk.columns else True\n",
    "        )\n",
    "        chunk = chunk[filters]\n",
    "        tract_field = 'census_tract_number' if 'census_tract_number' in chunk.columns else 'census_tract'\n",
    "        chunk = chunk.dropna(subset=[tract_field, 'loan_amount_000s', 'applicant_income_000s'])\n",
    "        chunk['year'] = year\n",
    "        chunk = chunk.rename(columns={tract_field: 'census_tract'})\n",
    "        existing_cols = [col for col in columns_to_keep if col in chunk.columns]\n",
    "        chunk = chunk[existing_cols]\n",
    "        chunk.to_csv(os.path.join(output_path, output_filename), mode='a', index=False, header=first_chunk)\n",
    "        first_chunk = False\n",
    "\n",
    "    print(f\"Finished {year}: saved to cleaned_data/{output_filename}\")\n",
    "\n",
    "# === Step 3: Special handling for 2023 data ===\n",
    "def process_2023_file(folder, output_filename):\n",
    "    chunk_size = 100000\n",
    "    first_chunk = True\n",
    "    colnames = [\n",
    "        \"as_of_year\", \"lei\", \"loan_type\", \"property_type\", \"loan_purpose\", \"occupancy\", \n",
    "        \"preapproval\", \"loan_amount\", \"action_taken\", \"state_abbr\", \"county_code\", \"census_tract\"\n",
    "    ]\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith(\".csv\") or file.endswith(\".txt\"):\n",
    "            path = os.path.join(folder, file)\n",
    "            break\n",
    "\n",
    "    for chunk in pd.read_csv(path, sep=\"|\", chunksize=chunk_size, header=None, low_memory=False):\n",
    "        sub = chunk.iloc[:, :12].copy()\n",
    "        sub.columns = colnames\n",
    "        sub = sub[sub['state_abbr'].isin(['FL', 'TX'])]\n",
    "        sub = sub[\n",
    "            (sub['loan_type'] == 1) &\n",
    "            (sub['property_type'] == 1) &\n",
    "            (sub['occupancy'] == 1) &\n",
    "            (sub['action_taken'] == 1)\n",
    "        ]\n",
    "        sub = sub.dropna(subset=['loan_amount', 'census_tract', 'county_code'])\n",
    "        sub['year'] = 2023\n",
    "        sub.rename(columns={'loan_amount': 'loan_amount_000s'}, inplace=True)\n",
    "        sub = sub[['year', 'state_abbr', 'county_code', 'census_tract', 'loan_amount_000s']]\n",
    "        sub.to_csv(os.path.join(output_path, output_filename), mode='a', index=False, header=first_chunk)\n",
    "        first_chunk = False\n",
    "\n",
    "    print(f\"Finished 2023: saved to cleaned_data/{output_filename}\")\n",
    "\n",
    "# === Step 4: Run All ===\n",
    "state_codes = ['12', '48']\n",
    "process_hmda_file(base_path + \"2007_data/\", \"cleaned_2007_FL_TX.csv\", 2007, \",\", state_codes)\n",
    "process_hmda_file(base_path + \"2017_data/\", \"cleaned_2017_FL_TX.csv\", 2017, \",\", state_codes)\n",
    "process_2023_file(base_path + \"2023_data/\", \"cleaned_2023_FL_TX.csv\")\n",
    "\n",
    "print(\"🎉 All files processed and saved in cleaned_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f468af6b-4581-48ac-9170-4dcf9aeff4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tract centroid coordinates saved to input_data1/tract_latlon_lookup.csv\n",
      "Skipped 2007: no usable tract column\n",
      "Skipped 2017: no usable tract column\n",
      "Merged tract coords with 2023 and saved to geo_cleaned_2023_FL_TX.csv\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import os\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# === Step 1: Unzip shapefiles ===\n",
    "def unzip_shapefile(zip_path, extract_to):\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "unzip_shapefile(\"input_data1/tl_2023_12_tract.zip\", \"input_data1/shapefiles/fl/\")\n",
    "unzip_shapefile(\"input_data1/tl_2023_48_tract.zip\", \"input_data1/shapefiles/tx/\")\n",
    "\n",
    "# === Step 2: Load shapefiles as GeoDataFrames ===\n",
    "gdf_fl = gpd.read_file(\"input_data1/shapefiles/fl\")\n",
    "gdf_tx = gpd.read_file(\"input_data1/shapefiles/tx\")\n",
    "\n",
    "# === Step 3: Combine and calculate centroids ===\n",
    "gdf_combined = pd.concat([gdf_fl, gdf_tx], ignore_index=True)\n",
    "gdf_combined = gdf_combined[[\"GEOID\", \"geometry\"]].copy()\n",
    "\n",
    "# Optional: fix centroid CRS warning (reproject to Web Mercator)\n",
    "gdf_combined = gdf_combined.to_crs(epsg=3857)\n",
    "\n",
    "gdf_combined_proj = gdf_combined.to_crs(epsg=3857)  # Web Mercator projection\n",
    "gdf_combined[\"centroid\"] = gdf_combined_proj.geometry.centroid\n",
    "gdf_combined[\"lat\"] = gdf_combined[\"centroid\"].y\n",
    "gdf_combined[\"lon\"] = gdf_combined[\"centroid\"].x\n",
    "gdf_combined[\"lat\"] = gdf_combined.centroid.y\n",
    "gdf_combined[\"lon\"] = gdf_combined.centroid.x\n",
    "gdf_combined.drop(columns=[\"geometry\", \"centroid\"], inplace=True)\n",
    "\n",
    "# === Step 4: Save tract-to-coordinates lookup table ===\n",
    "gdf_combined.to_csv(\"input_data1/tract_latlon_lookup.csv\", index=False)\n",
    "print(\"Tract centroid coordinates saved to input_data1/tract_latlon_lookup.csv\")\n",
    "\n",
    "# === Step 5: Merge lookup with HMDA data ===\n",
    "for year in [2007, 2017, 2023]:\n",
    "    fpath = f\"cleaned_data/cleaned_{year}_FL_TX.csv\"\n",
    "    if not os.path.exists(fpath):\n",
    "        print(f\"Skipped {year}: file not found at {fpath}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(fpath)\n",
    "\n",
    "    # Identify correct census tract column\n",
    "    tract_field = None\n",
    "    for candidate in ['census_tract', 'census_tract_number']:\n",
    "        if candidate in df.columns:\n",
    "            tract_field = candidate\n",
    "            break\n",
    "\n",
    "    if tract_field is None:\n",
    "        print(f\"Skipped {year}: no usable tract column\")\n",
    "        continue\n",
    "\n",
    "    df[tract_field] = df[tract_field].astype(str).str.zfill(11)\n",
    "    df = df.merge(gdf_combined, how='left', left_on=tract_field, right_on='GEOID')\n",
    "    df.to_csv(f\"cleaned_data/geo_cleaned_{year}_FL_TX.csv\", index=False)\n",
    "    print(f\"Merged tract coords with {year} and saved to geo_cleaned_{year}_FL_TX.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "771c5935-fd0c-4ce4-864e-6caf50015190",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cleaned_data/geo_cleaned_2023_FL_TX.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcleaned_data/geo_cleaned_2023_FL_TX.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Basic info\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39minfo())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cleaned_data/geo_cleaned_2023_FL_TX.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"cleaned_data/geo_cleaned_2023_FL_TX.csv\")\n",
    "\n",
    "# Basic info\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "\n",
    "# Missing data check\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Distribution of loan amounts\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(df['loan_amount_000s'], bins=50, kde=True)\n",
    "plt.title(\"Loan Amount Distribution\")\n",
    "plt.xlabel(\"Loan Amount ($000s)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Average loan size by county\n",
    "avg_by_county = df.groupby('county_code')['loan_amount_000s'].mean().sort_values(ascending=False)\n",
    "print(avg_by_county.head())\n",
    "\n",
    "# Scatter plot of lat/lon with loan amounts\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(\n",
    "    x=df[\"lon\"], \n",
    "    y=df[\"lat\"], \n",
    "    c=df[\"loan_amount_000s\"], \n",
    "    cmap=\"viridis\", \n",
    "    alpha=0.5,\n",
    "    s=50  # Adjust point size as needed\n",
    ")\n",
    "plt.colorbar(label=\"Loan Amount ($000s)\")\n",
    "plt.title(\"Geospatial Loan Distribution (FL + TX)\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eecbcad-f71f-4112-8d3b-cffbeed4426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Project/\n",
    "├── cleaned_data/\n",
    "│   ├── geo_cleaned_2023_FL_TX.csv\n",
    "│   ├── cleaned_2007_FL_TX.csv\n",
    "│   ├── cleaned_2017_FL_TX.csv\n",
    "├── input_data1/\n",
    "│   ├── hmda_*.zip (raw HMDA files)\n",
    "│   ├── tl_2023_12_tract.zip\n",
    "│   ├── tl_2023_48_tract.zip\n",
    "│   ├── shapefiles/\n",
    "│       ├── fl/\n",
    "│       ├── tx/\n",
    "│   ├── tract_latlon_lookup.csv\n",
    "├── notebooks/\n",
    "│   ├── `project_data_and_eda.ipynb`   ← ⬅️ MOVE YOUR CURRENT WORK HERE\n",
    "├── scripts/\n",
    "│   ├── clean_hmda_data.py\n",
    "│   ├── add_latlon_from_tracts.py\n",
    "├── README.md"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
